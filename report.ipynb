{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from functools import partial as par\n",
    "import functools as ft\n",
    "import time\n",
    "from ctypes import c_void_p, Structure, c_double, c_int, cdll, cast, POINTER\n",
    "from numpy.ctypeslib import ndpointer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "def F(*z):\n",
    "    z = [*z]\n",
    "    z[0] = [z[0]]\n",
    "    return [*ft.reduce(lambda x, y: map(y, x), z)][0]\n",
    "FF = lambda *z: [*ft.reduce(lambda x, y: map(y, x), z)]\n",
    "fyx = lambda f, *x: lambda *y: f(*y, *x)\n",
    "\n",
    "lib = cdll.LoadLibrary(\"./a.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"sp500.h5\")\n",
    "temp = df[\"volume\"].groupby(level = 1)\\\n",
    "                    .apply(lambda x: x.ewm(halflife = 60).mean());\n",
    "df.loc[temp.index, \"naive\"] = temp.groupby(level = 1).shift(1);\n",
    "# print(df.loc[pd.IndexSlice[\"2000-01-01\":\"2000-01-10\", \"AAPL\"], :])\n",
    "# -> Appendix A1\n",
    "window = 5\n",
    "for i in range(window + 1):\n",
    "\tname = f\"k{i}\"\n",
    "\tdf[name] = (df[\"volume\"].groupby(level = 1).shift(i)\n",
    "                / df[\"naive\"] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSlice = pd.IndexSlice[:\"2013-01-01\", :]\n",
    "valSlice = pd.IndexSlice[\"2013-01-01\":\"2015-01-01\":, :]\n",
    "tvSlice = pd.IndexSlice[:\"2015-01-01\", :]\n",
    "testSlice = pd.IndexSlice[\"2015-01-01\":, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd2np(df, Slice, window, permutation = False):\n",
    "\tdf_nona = df.loc[Slice].dropna(\n",
    "\t\tsubset = [f\"k{i}\" for i in range(window + 1)]\n",
    "\t)\n",
    "\tuspn = list(set(df_nona.index.get_level_values(\"uspn\").to_list()))\n",
    "\tuspn.sort()\n",
    "\tprint(len(uspn))\n",
    "\tX, Y, w = [], [], []\n",
    "\tfor x in uspn:\n",
    "\t\ttable = df_nona.loc[\n",
    "\t\t\tpd.IndexSlice[:, x],\n",
    "\t\t\t[\"sp_weight\"] + [f\"k{i}\" for i in range(window + 1)]\n",
    "\t\t]\n",
    "\t\tw.append(table[\"sp_weight\"].to_numpy())\n",
    "\t\tY.append(table[\"k0\"].to_numpy())\n",
    "\t\tX.append(np.array([\n",
    "\t\t\ttable[f\"k{i + 1}\"].to_numpy() for i in range(window)\n",
    "\t\t]).T)\n",
    "\tX = np.concatenate(X)\n",
    "\tY = np.concatenate(Y)\n",
    "\tw = np.concatenate(w)\n",
    "\ttot = w.shape[0]\n",
    "\tprint(tot)\n",
    "\tif permutation:\n",
    "\t\tperm = F(tot, range, np.random.permutation)\n",
    "\t\tw = w[perm]\n",
    "\t\tX = X[perm]\n",
    "\t\tY = Y[perm]\n",
    "\treturn Y, X, w, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n",
      "1388738\n",
      "484\n",
      "240565\n"
     ]
    }
   ],
   "source": [
    "Y1, X1, w1, step1 = pd2np(df, trainSlice, 5, True)\n",
    "Y2, X2, w2, step2 = pd2np(df, valSlice, 5, True)\n",
    "X = np.concatenate((X1, X2))\n",
    "Y = np.concatenate((Y1, Y2))\n",
    "w = np.concatenate((w1, w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_wls_iter(\n",
    "    y_next, x_next, w_next, n2, m,\n",
    "    x = np.array([]), xTwx = None, xTwy = None,\n",
    "    n1 = 0, update = False\n",
    "):\n",
    "    tot_len = n1 + 2 * n2 + m * (m + 2)\n",
    "    if xTwx is None:\n",
    "        xTwx = np.zeros((m, m))\n",
    "    if xTwy is None:\n",
    "        xTwy = np.zeros((m, 1))\n",
    "    double = lambda x: x.astype('d')\n",
    "    x_next = double(x_next)\n",
    "    w_next = double(w_next)\n",
    "    y_next = double(y_next)\n",
    "    x = double(x)\n",
    "    xTwx = double(xTwx)\n",
    "    xTwy = double(xTwy)\n",
    "    lib.wls_iter.restype = ndpointer(dtype = c_double, shape = (tot_len,))\n",
    "    results = lib.wls_iter(\n",
    "        c_void_p(x.ctypes.data),\n",
    "        c_void_p(xTwx.ctypes.data),\n",
    "        c_void_p(xTwy.ctypes.data),\n",
    "        c_void_p(x_next.ctypes.data),\n",
    "        c_void_p(w_next.ctypes.data),\n",
    "        c_void_p(y_next.ctypes.data),\n",
    "        n1, n2, m, update\n",
    "    )\n",
    "    xTwx = results[:m * m].reshape(m, m)\n",
    "    xTwy = results[m * m : m * (m + 1)]\n",
    "    predict = results[m * (m + 1) : m * (m + 2)]\n",
    "    yhat_next = results[m * (m + 2) : m * (m + 2) + n2]\n",
    "    if update:\n",
    "        yhat = results[-n1 - n2:]\n",
    "    else:\n",
    "        yhat = None\n",
    "    return xTwx, xTwy, predict, yhat_next, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 WLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.176\n",
      "Model:                            WLS   Adj. R-squared (uncentered):              0.176\n",
      "Method:                 Least Squares   F-statistic:                          5.943e+04\n",
      "Date:                Fri, 24 Feb 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        01:41:27   Log-Likelihood:                     -1.8601e+06\n",
      "No. Observations:             1388738   AIC:                                  3.720e+06\n",
      "Df Residuals:                 1388733   BIC:                                  3.720e+06\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.4255      0.001    415.833      0.000       0.423       0.427\n",
      "x2             0.0745      0.001     66.428      0.000       0.072       0.077\n",
      "x3             0.0510      0.001     45.011      0.000       0.049       0.053\n",
      "x4             0.0388      0.001     34.072      0.000       0.037       0.041\n",
      "x5             0.0465      0.001     43.194      0.000       0.044       0.049\n",
      "==============================================================================\n",
      "Omnibus:                  4923880.902   Durbin-Watson:                   2.001\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):   17568577162529.531\n",
      "Skew:                          69.613   Prob(JB):                         0.00\n",
      "Kurtosis:                   17427.081   Cond. No.                         2.15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "0.4254561447984051\n",
      "0.07449455035495588\n",
      "0.05101702082718002\n",
      "0.038798740296170725\n",
      "0.04647869400072205\n",
      "Time Usage\n",
      "statsmodels WLS 1388738: 0.4589822292327881\n",
      "C code 1388738: 0.20333409309387207\n",
      "We get the same results for 2 methods above.\n",
      "                                 WLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.170\n",
      "Model:                            WLS   Adj. R-squared (uncentered):              0.170\n",
      "Method:                 Least Squares   F-statistic:                          6.652e+04\n",
      "Date:                Fri, 24 Feb 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        01:41:27   Log-Likelihood:                     -2.1795e+06\n",
      "No. Observations:             1629303   AIC:                                  4.359e+06\n",
      "Df Residuals:                 1629298   BIC:                                  4.359e+06\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.4219      0.001    440.277      0.000       0.420       0.424\n",
      "x2             0.0733      0.001     69.923      0.000       0.071       0.075\n",
      "x3             0.0487      0.001     45.968      0.000       0.047       0.051\n",
      "x4             0.0425      0.001     39.920      0.000       0.040       0.045\n",
      "x5             0.0459      0.001     45.653      0.000       0.044       0.048\n",
      "==============================================================================\n",
      "Omnibus:                  5944430.299   Durbin-Watson:                   2.001\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):   25114920810012.258\n",
      "Skew:                          76.002   Prob(JB):                         0.00\n",
      "Kurtosis:                   19236.443   Cond. No.                         2.14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "0.42188812965748956\n",
      "0.07333454760039705\n",
      "0.04870767000920516\n",
      "0.0424941070880122\n",
      "0.04590773404302659\n",
      "statsmodels WLS 1629303: 0.5535001754760742\n",
      "C code next 240565: 0.038729190826416016\n",
      "We get the same results for 2 methods above.\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "test = sm.WLS(Y1, X1, weights = w1, missing = \"drop\").fit()\n",
    "print(test.summary())\n",
    "time1 = time.time()\n",
    "results = c_wls_iter(Y1, X1, w1, step1, window)\n",
    "xTwx = results[0]\n",
    "xTwy = results[1]\n",
    "[print(x) for x in results[2]]\n",
    "time2 = time.time()\n",
    "print(\"Time Usage\")\n",
    "print(f\"statsmodels WLS {step1}:\", time1 - time0)\n",
    "print(f\"C code {step1}:\", time2 - time1)\n",
    "print(\"We get the same results for 2 methods above.\")\n",
    "test = sm.WLS(Y, X, weights = w, missing = \"drop\").fit()\n",
    "print(test.summary())\n",
    "time3 = time.time()\n",
    "results = c_wls_iter(Y2, X2, w2, step2, window,\n",
    "                        xTwx = xTwx, xTwy = xTwy, n1 = step1)\n",
    "[print(x) for x in results[2]]\n",
    "time4 = time.time()\n",
    "print(f\"statsmodels WLS {step1 + step2}:\", time3 - time2)\n",
    "print(f\"C code next {step2}:\", time4 - time3)\n",
    "print(\"We get the same results for 2 methods above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2769c8024000d2372e0e7315aa533eee97b6855646f860882228e565a3a4f8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
