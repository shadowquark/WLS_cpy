{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from functools import partial as par\n",
    "import functools as ft\n",
    "import time\n",
    "from ctypes import c_void_p, Structure, c_double, c_int, cdll, cast, POINTER\n",
    "from numpy.ctypeslib import ndpointer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "def F(*z):\n",
    "    z = [*z]\n",
    "    z[0] = [z[0]]\n",
    "    return [*ft.reduce(lambda x, y: map(y, x), z)][0]\n",
    "FF = lambda *z: [*ft.reduce(lambda x, y: map(y, x), z)]\n",
    "fyx = lambda f, *x: lambda *y: f(*y, *x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Problem\n",
    "We want to realize an online algorithm to keep updating the results of linear regression. When we know the regression results of n1 samples and receive another n2 samples, we don't need to recalculate the linear regression of $n_1+ n_2$ samples, but merely update the old results. This is very useful when $n_1$ is large and $n_2$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Formula\n",
    "After calculating the following linear regression for the first $n_1$ samples,\n",
    "$$\\hat{y} =x(x^Twx)^{-1}x^Twy,$$\n",
    "where $y$ and $\\hat{y}$ are $(n_1, 1)$ matrix, $x$ is $(n_1, m)$ matrix, and $w$ is diagonalized $(n_1, n_1)$ matrix, we know $x^Twx$ and $x^Twy$ are 2 $(m, m)$ matrix, which are small compared to $x$, $y$, $\\hat{y}$, and $w$, since their sizes are proportional to $n_1$. Therefore, we can store their values for calculation in the next step. Now consider $(n_2, m)$ matrix $\\tilde{x}$, $(n_2, 1)$ matrix $\\tilde{y}$, and diagonalized $(n_2, n_2)$ matrix $\\tilde{w}$ are added for linear regression.\n",
    "$$\\hat{y'} =\\begin{pmatrix}x\\\\\\tilde{x}\\end{pmatrix}\n",
    "            \\left(\\begin{pmatrix}x^T & \\tilde{x}^T\\end{pmatrix}\n",
    "                    \\begin{pmatrix}w & 0 \\\\ 0 & \\tilde{w}\\end{pmatrix}\n",
    "                    \\begin{pmatrix}x\\\\ \\tilde{x}\\end{pmatrix}\\right)^{-1}\n",
    "            \\begin{pmatrix}x^T & \\tilde{x}^T\\end{pmatrix}\n",
    "            \\begin{pmatrix}w & 0 \\\\ 0 & \\tilde{w}\\end{pmatrix}\n",
    "            \\begin{pmatrix}y\\\\\\tilde{y}\\end{pmatrix}$$\n",
    "$$= \\begin{pmatrix}x\\\\\\tilde{x}\\end{pmatrix}(x^Twx + \\tilde{x}^T\\tilde{w}\\tilde{x})^{-1}(x^Twy + \\tilde{x}^T\\tilde{w}\\tilde{y}),$$\n",
    "$$= \\begin{pmatrix}x\\\\\\tilde{x}\\end{pmatrix}(A+B)^{-1}(C+D)$$\n",
    "Since we've already known the values of A and C, their calculation complexity is $O(1)$. B is $(m, n_2) * (n_2, n_2) * (n_2, m)$. Normally, it costs $O(m n_2^2)$. But we know $\\tilde{w}$ is diagonalized, and hence can optimized it to $O(mn_2)$. Similarly, D can be optimized to $O(n_2)$. Additionally A+B is $O(m^2)$, C+D is $O(m)$, (A+B)$^{-1}$ is ~$O(m^3)$ (note that the modern inversion algorithms are faster than $O(m^3)$), and (A+B)$^{-1}$(C+D) is $O(m^2)$. Therefore, the total complexity is $O(max(m^3, mn_2))$. Finally, we find a way to calculate new regression irrelevant with $n_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming\n",
    "### The programming section has 4 parts.\n",
    "1. Realize the algorithm above in `C`\n",
    "2. Compile `C` program to make it linkable in `Python` \n",
    "4. Wrap `C` function in `Python`.\n",
    "3. Use random data or `sp500.h5` data to test and benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `C` Code\n",
    "The `C` code is in `calculator.c`. The functions of `C` are \n",
    "```\n",
    "double *inverse(double *x, int m); // A(m, m)^{-1}\n",
    "double *add(double *x, double *y, int m, int n); // A(m, m) + B(m, m)\n",
    "double *times(double *x, double *y, int m, int p, int n); // A(m, p) * B(p, n)\n",
    "// The test for these basic functions wrapped in python can be found in benchmark.py\n",
    "\n",
    "double *calc_xTw(double *x, double *w, int m, int n); // x^T(m, n) * w(n, n)\n",
    "// We avoid using times to calculate xTw, since we know w is diagonalized.\n",
    "// Additionally, we never calculate x^T, and merely use the value from the correct position.\n",
    "// Both these strategies make the calculation faster and save RAM from creating a (n, n) matrix.\n",
    "\n",
    "double *wls_iter(double *x, double *xTwx, double *xTwy,\n",
    "                    double *x_next, double *w_next, double *y_next,\n",
    "                    int n1, int n2, int m, bool update);\n",
    "// wls_iter is the linear regression\n",
    "// xTwx and xTwy are from the regression of n1\n",
    "// x_next, y_next, w_next are used to calculate the regression of n1 + n2\n",
    "// if update = false, only yhat of n2 samples are calculated.\n",
    "// if update = true, yhat of all n1 + n2 samples are calculated.\n",
    "// This is the only case that the time complexity of wls_iter depends on n1.\n",
    "// Therefore, except that we really want to know the whole and new yhat, we should set it to false.\n",
    "\n",
    "int main();\n",
    "// The main program has a very large loop to test whether wls_iter free all internal memory usage correctly.\n",
    "// To run it, gcc calculator.c -llapack and ./a.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile `C` Code to make it linkable\n",
    "We need to install `lapack` first. It is easy to find in package manager of Linux/Homebrew/Windows Subsystem for Linux. Since we use `lapack`, it should be linked while compiling. Therefore, we use\n",
    "```\n",
    "gcc -fPIC -llapack -shared -o a.so calculator.c\n",
    "```\n",
    "There is a tricky thing requiring special notification. When `Python` loads `C` functions from `.so`, it loads them separately. In other words, if `C` function1 calls `C` function2, the communication between 2 functions may fail after wrapping to `cdll` in `Python`. Therefore, we need to copy all `C` functions required by `C` function1 inside `C` function1, so that `C` function1 does not require any functions outside. After avoiding this trouble, we can load `C` functions as library of `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = cdll.LoadLibrary(\"./a.so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap `C` function to `Python` function\n",
    "Here we only focus on the weighted linear regression. Tests on other `C` functions can be found in `benchmark.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call `C` function, we need to carefully assign the value types of all parameters, since this is the most important difference between `C` and `Python`. `y_next` is $\\tilde{y}$, `x_next` is $\\tilde{x}$, and `w_next` is $\\tilde{w}$. The return value is `double*`, and we save it to a `numpy` array. `C` can only return one single value. Therefore, we need to separate this value to different `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_wls_iter(\n",
    "    y_next, x_next, w_next, n2, m,\n",
    "    x = np.array([]), xTwx = None, xTwy = None,\n",
    "    n1 = 0, update = False\n",
    "):\n",
    "    tot_len = n1 + 2 * n2 + m * (m + 2)\n",
    "    if xTwx is None:\n",
    "        xTwx = np.zeros((m, m))\n",
    "    if xTwy is None:\n",
    "        xTwy = np.zeros((m, 1))\n",
    "    double = lambda x: x.astype('d')\n",
    "    x_next = double(x_next)\n",
    "    w_next = double(w_next)\n",
    "    y_next = double(y_next)\n",
    "    x = double(x)\n",
    "    xTwx = double(xTwx)\n",
    "    xTwy = double(xTwy)\n",
    "    lib.wls_iter.restype = ndpointer(dtype = c_double, shape = (tot_len,))\n",
    "    results = lib.wls_iter(\n",
    "        c_void_p(x.ctypes.data),\n",
    "        c_void_p(xTwx.ctypes.data),\n",
    "        c_void_p(xTwy.ctypes.data),\n",
    "        c_void_p(x_next.ctypes.data),\n",
    "        c_void_p(w_next.ctypes.data),\n",
    "        c_void_p(y_next.ctypes.data),\n",
    "        n1, n2, m, update\n",
    "    )\n",
    "    xTwx = results[:m * m].reshape(m, m)\n",
    "    xTwy = results[m * m : m * (m + 1)]\n",
    "    predict = results[m * (m + 1) : m * (m + 2)]\n",
    "    if update:\n",
    "        yhat = results[-n1 - n2:]\n",
    "    else:\n",
    "        yhat = None\n",
    "    return xTwx, xTwy, predict, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test whether our algorithm is correct and benchmark it\n",
    "We have 2 dataset for benchmark.\n",
    "1. Test on random number\n",
    "2. Test on `SP500` data\n",
    "\n",
    "From the benchmark, we want to confirm 3 things.\n",
    "1. The results we get are correct compared to other methods.\n",
    "2. Coding in `C` is better when comparing to use `numpy` operation directly, although the backend of `numpy` is `C` as well.\n",
    "3. Our algorithm is indeed online calculation, which avoid recalculating the whole samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 WLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.701\n",
      "Model:                            WLS   Adj. R-squared (uncentered):              0.701\n",
      "Method:                 Least Squares   F-statistic:                          2.297e+04\n",
      "Date:                Tue, 28 Feb 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:58:36   Log-Likelihood:                         -20395.\n",
      "No. Observations:               49000   AIC:                                  4.080e+04\n",
      "Df Residuals:                   48995   BIC:                                  4.084e+04\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.1908      0.004     43.104      0.000       0.182       0.199\n",
      "x2             0.1865      0.004     42.204      0.000       0.178       0.195\n",
      "x3             0.1827      0.004     41.245      0.000       0.174       0.191\n",
      "x4             0.1873      0.004     42.054      0.000       0.179       0.196\n",
      "x5             0.1845      0.004     41.506      0.000       0.176       0.193\n",
      "==============================================================================\n",
      "Omnibus:                       86.225   Durbin-Watson:                   1.970\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               75.607\n",
      "Skew:                           0.048   Prob(JB):                     3.82e-17\n",
      "Kurtosis:                       2.834   Cond. No.                         4.03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "x1: 0.19077602363228363\n",
      "x2: 0.1864943830906497\n",
      "x3: 0.18269859659656973\n",
      "x4: 0.18732488453632667\n",
      "x5: 0.18449136832415824\n",
      "[[0.19077602]\n",
      " [0.18649438]\n",
      " [0.1826986 ]\n",
      " [0.18732488]\n",
      " [0.18449137]]\n",
      "                                 WLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.701\n",
      "Model:                            WLS   Adj. R-squared (uncentered):              0.701\n",
      "Method:                 Least Squares   F-statistic:                          2.347e+04\n",
      "Date:                Tue, 28 Feb 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:58:44   Log-Likelihood:                         -20779.\n",
      "No. Observations:               50000   AIC:                                  4.157e+04\n",
      "Df Residuals:                   49995   BIC:                                  4.161e+04\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.1913      0.004     43.691      0.000       0.183       0.200\n",
      "x2             0.1863      0.004     42.612      0.000       0.178       0.195\n",
      "x3             0.1823      0.004     41.624      0.000       0.174       0.191\n",
      "x4             0.1874      0.004     42.525      0.000       0.179       0.196\n",
      "x5             0.1846      0.004     41.954      0.000       0.176       0.193\n",
      "==============================================================================\n",
      "Omnibus:                       88.363   Durbin-Watson:                   1.972\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               77.490\n",
      "Skew:                           0.049   Prob(JB):                     1.49e-17\n",
      "Kurtosis:                       2.834   Cond. No.                         4.03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "x1: 0.1913071306937053\n",
      "x2: 0.1862679763827041\n",
      "x3: 0.18233763677221992\n",
      "x4: 0.18741970751744375\n",
      "x5: 0.1845570819747473\n",
      "[[0.19130713]\n",
      " [0.18626798]\n",
      " [0.18233764]\n",
      " [0.18741971]\n",
      " [0.18455708]]\n",
      "\n",
      "Time Usage\n",
      "statsmodels WLS 49000: 0.05563068389892578\n",
      "C code 49000: 0.007581949234008789\n",
      "numpy 49000: 8.040099859237671\n",
      "We get the same results for 3 methods above.\n",
      "statsmodels WLS 50000: 0.02528238296508789\n",
      "C code next 1000: 0.0009584426879882812\n",
      "numpy next 1000: 0.002610445022583008\n",
      "We get the same results for 3 methods above.\n"
     ]
    }
   ],
   "source": [
    "# Generate Random Number\n",
    "m, n = 5, 50000\n",
    "step = 49000\n",
    "X = np.random.rand(n, m)\n",
    "X1, X2 = X[:step, :], X[step:, :]\n",
    "Y = np.random.rand(n)\n",
    "Y1, Y2 = Y[:step], Y[step:]\n",
    "w = np.random.rand(n)\n",
    "w1, w2 = w[:step], w[step:]\n",
    "# statsmodels\n",
    "time0 = time.time()\n",
    "test = sm.WLS(Y1, X1, weights = w1, missing = \"drop\").fit()\n",
    "print(test.summary())\n",
    "# my C code\n",
    "time1 = time.time()\n",
    "results = c_wls_iter(Y1, X1, w1, step, m)\n",
    "xTwx, xTwy = results[0], results[1]\n",
    "[print(f\"x{i + 1}: {x}\") for i, x in enumerate(results[2])]\n",
    "# numpy matrix operation\n",
    "time2 = time.time()\n",
    "test1 = np.matmul(np.matmul(X1.T, np.diag(w1)), X1)\n",
    "test2 = np.matmul(np.matmul(X1.T, np.diag(w1)), Y1.reshape(-1, 1))\n",
    "print(np.matmul(np.linalg.inv(test1), test2))\n",
    "# statsmodels\n",
    "time3 = time.time()\n",
    "test = sm.WLS(Y, X, weights = w, missing = \"drop\").fit()\n",
    "print(test.summary())\n",
    "# my C code\n",
    "time4 = time.time()\n",
    "results = c_wls_iter(Y2, X2, w2, n - step, m,\n",
    "                        xTwx = xTwx, xTwy = xTwy, n1 = step)\n",
    "[print(f\"x{i + 1}: {x}\") for i, x in enumerate(results[2])]\n",
    "# numpy matrix operation\n",
    "time5 = time.time()\n",
    "test1 = xTwx + np.matmul(np.matmul(X2.T, np.diag(w2)), X2)\n",
    "test2 = xTwy.reshape(-1, 1)\\\n",
    "        + np.matmul(np.matmul(X2.T, np.diag(w2)), Y2.reshape(-1, 1))\n",
    "print(np.matmul(np.linalg.inv(test1), test2))\n",
    "time6 = time.time()\n",
    "print(\"\\nTime Usage\")\n",
    "print(f\"statsmodels WLS {step}:\", time1 - time0)\n",
    "print(f\"C code {step}:\", time2 - time1)\n",
    "print(f\"numpy {step}:\", time3 - time2)\n",
    "print(\"We get the same results for 3 methods above.\")\n",
    "print(f\"statsmodels WLS {n}:\", time4 - time3)\n",
    "print(f\"C code next {n - step}:\", time5 - time4)\n",
    "print(f\"numpy next {n - step}:\", time6 - time5)\n",
    "print(\"We get the same results for 3 methods above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` is always the slowest. Although `numpy` is based on `C` as well, the problems of `numpy` are obvious. \n",
    "1. We use `double*` directly in our `C` function. However `numpy` require more complex data structure to apply all its internal operations. This can be read from that if $n$ is small, `numpy` is not extremely slower than our methods. In contrast, if $n$ is very large, `numpy` requires much larger RAM and turns extremely slower as well.\n",
    "2. When using times operation of matrices to calculate $x^Twx$ and $x^Twy$, $w$ is a $(n, n)$ matrix. Compare with our algorithm, the time and space complexity here are obvious much higher.\n",
    "3. The advantage of our algorithm comes from not only `C` language, but also specially designed calculation with strong flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `SP500` data to test the results and benchmark the speed\n",
    "we add window + 1 ($m$ = window) columns in the DataFrame fist. One is the target $y$=vlm/naive - 1, the other columns contain the information from past used for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"sp500.h5\")\n",
    "temp = df[\"volume\"].groupby(level = 1)\\\n",
    "                    .apply(lambda x: x.ewm(halflife = 60).mean());\n",
    "df.loc[temp.index, \"naive\"] = temp.groupby(level = 1).shift(1);\n",
    "window = 5\n",
    "for i in range(window + 1):\n",
    "\tname = f\"k{i}\"\n",
    "\tdf[name] = (df[\"volume\"].groupby(level = 1).shift(i)\n",
    "                / df[\"naive\"] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use trainSlice as $n_1$ part, and valSlice as $n_2$ part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSlice = pd.IndexSlice[:\"2013-01-01\", :]\n",
    "valSlice = pd.IndexSlice[\"2013-01-01\":\"2015-01-01\":, :]\n",
    "tvSlice = pd.IndexSlice[:\"2015-01-01\", :]\n",
    "testSlice = pd.IndexSlice[\"2015-01-01\":, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to convert `pandas` to `numpy` for the communication with `double*` in `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd2np(df, Slice, window, permutation = False):\n",
    "    YXw = [f\"k{i}\" for i in range(window + 1)] + [\"sp_weight\"]\n",
    "    df_nona = df.loc[Slice, YXw].dropna()\n",
    "    df_nona = df_nona.values\n",
    "    tot = df_nona.shape[0]\n",
    "    if permutation:\n",
    "        perm = F(tot, range, np.random.permutation)\n",
    "        df_nona = df_nona[perm]\n",
    "    return df_nona[:, 0], df_nona[:, 1:-1], df_nona[:, -1], tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1, X1, w1, step1 = pd2np(df, trainSlice, 5, True)\n",
    "Y2, X2, w2, step2 = pd2np(df, valSlice, 5, True)\n",
    "X = np.concatenate((X1, X2))\n",
    "Y = np.concatenate((Y1, Y2))\n",
    "w = np.concatenate((w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the benchmark in 2 cases.\n",
    "1. Calculating linear regression of trainSlice\n",
    "2. Calculation linear regression of trainSlice + valSlice with offline or online calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 WLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.176\n",
      "Model:                            WLS   Adj. R-squared (uncentered):              0.176\n",
      "Method:                 Least Squares   F-statistic:                          5.943e+04\n",
      "Date:                Tue, 28 Feb 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:58:56   Log-Likelihood:                     -1.8601e+06\n",
      "No. Observations:             1388738   AIC:                                  3.720e+06\n",
      "Df Residuals:                 1388733   BIC:                                  3.720e+06\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.4255      0.001    415.833      0.000       0.423       0.427\n",
      "x2             0.0745      0.001     66.428      0.000       0.072       0.077\n",
      "x3             0.0510      0.001     45.011      0.000       0.049       0.053\n",
      "x4             0.0388      0.001     34.072      0.000       0.037       0.041\n",
      "x5             0.0465      0.001     43.194      0.000       0.044       0.049\n",
      "==============================================================================\n",
      "Omnibus:                  4923880.902   Durbin-Watson:                   1.998\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):   17568577162529.551\n",
      "Skew:                          69.613   Prob(JB):                         0.00\n",
      "Kurtosis:                   17427.081   Cond. No.                         2.15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "x1: 0.42545614479841565\n",
      "x2: 0.07449455035494568\n",
      "x3: 0.051017020827160954\n",
      "x4: 0.038798740296179204\n",
      "x5: 0.04647869400072585\n",
      "                                 WLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.170\n",
      "Model:                            WLS   Adj. R-squared (uncentered):              0.170\n",
      "Method:                 Least Squares   F-statistic:                          6.652e+04\n",
      "Date:                Tue, 28 Feb 2023   Prob (F-statistic):                        0.00\n",
      "Time:                        16:58:57   Log-Likelihood:                     -2.1795e+06\n",
      "No. Observations:             1629303   AIC:                                  4.359e+06\n",
      "Df Residuals:                 1629298   BIC:                                  4.359e+06\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.4219      0.001    440.277      0.000       0.420       0.424\n",
      "x2             0.0733      0.001     69.923      0.000       0.071       0.075\n",
      "x3             0.0487      0.001     45.968      0.000       0.047       0.051\n",
      "x4             0.0425      0.001     39.920      0.000       0.040       0.045\n",
      "x5             0.0459      0.001     45.653      0.000       0.044       0.048\n",
      "==============================================================================\n",
      "Omnibus:                  5944430.299   Durbin-Watson:                   1.998\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):   25114920810012.199\n",
      "Skew:                          76.002   Prob(JB):                         0.00\n",
      "Kurtosis:                   19236.443   Cond. No.                         2.14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "x1: 0.42188812965749806\n",
      "x2: 0.07333454760038882\n",
      "x3: 0.04870767000918785\n",
      "x4: 0.04249410708801997\n",
      "x5: 0.045907734043029724\n",
      "\n",
      "Time Usage\n",
      "statsmodels WLS 1388738: 0.485628604888916\n",
      "C code 1388738: 0.2152256965637207\n",
      "We get the same results for 2 methods above.\n",
      "statsmodels WLS 1629303: 0.5717928409576416\n",
      "C code next 240565: 0.026180505752563477\n",
      "We get the same results for 2 methods above.\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "test = sm.WLS(Y1, X1, weights = w1, missing = \"drop\").fit()\n",
    "print(test.summary())\n",
    "time1 = time.time()\n",
    "results = c_wls_iter(Y1, X1, w1, step1, window)\n",
    "xTwx = results[0]\n",
    "xTwy = results[1]\n",
    "[print(f\"x{i + 1}: {x}\") for i, x in enumerate(results[2])]\n",
    "time2 = time.time()\n",
    "test = sm.WLS(Y, X, weights = w, missing = \"drop\").fit()\n",
    "print(test.summary())\n",
    "time3 = time.time()\n",
    "results = c_wls_iter(Y2, X2, w2, step2, window,\n",
    "                        xTwx = xTwx, xTwy = xTwy, n1 = step1)\n",
    "[print(f\"x{i + 1}: {x}\") for i, x in enumerate(results[2])]\n",
    "time4 = time.time()\n",
    "print(\"\\nTime Usage\")\n",
    "print(f\"statsmodels WLS {step1}:\", time1 - time0)\n",
    "print(f\"C code {step1}:\", time2 - time1)\n",
    "print(\"We get the same results for 2 methods above.\")\n",
    "print(f\"statsmodels WLS {step1 + step2}:\", time3 - time2)\n",
    "print(f\"C code next {step2}:\", time4 - time3)\n",
    "print(\"We get the same results for 2 methods above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `statsmodels` calculate more than just linear regression, it is hard to compare our methods directly with `statsmodels`. However, when we consider _update_ the old calculation (trainSlice) to a new result (trainSlice + valSlice), our method is clearly much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example for real world usage\n",
    "Keep updating the regression results everyday with reweighting newer samples to more important. To reweight the sample, we use a parameter called `halflife` describing how many days the weight of a sample will decay to half of the original values with fixed decaying ratio. Therefore, after `t` days, the weight of a sample decays to the following value.\n",
    "$$w' = w * 2^{-\\frac{t}{halflife}}$$\n",
    "Additionally, the decaying ratio everyday is $2^{-1/halflife}$. Therefore, when we calculating $x^Twx + \\tilde{x}^T\\tilde{w}\\tilde{x}$ for the next day, the first term will times a decaying ratio to $x^Twx * 2^{-1/halflife} +  \\tilde{x}^T\\tilde{w}\\tilde{x}$. The same change should apply to $x^Twy$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_regress(Y, X, w, halflife, ts_group = \"date\"):\n",
    "    ts = Y.index.get_level_values(ts_group).drop_duplicates()\n",
    "    tot, window = 0, len(X.columns)\n",
    "    print(window)\n",
    "    for date in ts:\n",
    "        step = Y.loc[date].size\n",
    "        _Y = Y.loc[date].values\n",
    "        _X = X.loc[date].values\n",
    "        _w = w.loc[date].values\n",
    "        if tot:\n",
    "            pass\n",
    "        else:\n",
    "            results = c_wls_iter(_Y, _X, _w, step, window)\n",
    "#xTwx, xTwy, predict, yhat_next, yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd2np(df, Slice, window, permutation = False):\n",
    "    YXw = [f\"k{i}\" for i in range(window + 1)] + [\"sp_weight\"]\n",
    "    df_nona = df.loc[Slice, YXw].dropna()\n",
    "    df_nona = df_nona.values\n",
    "    tot = df_nona.shape[0]\n",
    "    if permutation:\n",
    "        perm = F(tot, range, np.random.permutation)\n",
    "        df_nona = df_nona[perm]\n",
    "    return df_nona[:, 0], df_nona[:, 1:-1], df_nona[:, -1], tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "YXw = [f\"k{i}\" for i in range(window + 1)] + [\"sp_weight\"]\n",
    "YXwDF = df.loc[:, YXw].dropna()\n",
    "Y = YXwDF[\"k0\"]\n",
    "X = YXwDF[[f\"k{i + 1}\" for i in range(window)]]\n",
    "w = YXwDF[\"sp_weight\"]\n",
    "halflife = 250\n",
    "causal_regress(Y, X, w, halflife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2769c8024000d2372e0e7315aa533eee97b6855646f860882228e565a3a4f8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
